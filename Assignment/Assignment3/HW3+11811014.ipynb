{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS306 Homework3 Cluster\n",
    "\n",
    "### Information\n",
    "Author: 王超逸 WANG Chaoyi\n",
    "SID: 11811014\n",
    "\n",
    "### README: \n",
    "My work for the two tasks are in the same notebook file. The implementation of the latter task depends on the necessary imports of the former one and is tested to be all right, so please ensure the step of necessary imports has run when some exceptions occured in task2 if the two tasks were graded separately.\n",
    "\n",
    "### Reference:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task1: Mystery Data\n",
    "### Necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn as sk\n",
    "import yellowbrick as yb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data, necessary cleaning and preview "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./HW3_1_data.csv')\n",
    "# Remove null values (in fact there is not null value);\n",
    "df.dropna(axis=0, inplace=True)\n",
    "# Remove duplicates (in fact, no either);\n",
    "df = df.drop_duplicates()\n",
    "# Briefly check the data\n",
    "df.info()\n",
    "df.describe(include='all')\n",
    "# df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(20,20))\n",
    "for i ,col in enumerate(df.columns):\n",
    "    ax = f.add_subplot(6,3,i+1)\n",
    "    sns.distplot(df[col].ffill(),kde=False)\n",
    "    ax.set_title('discribution',color = 'blue')\n",
    "    plt.ylabel('discribution')\n",
    "    plt.show()\n",
    "f.tight_layout()"
   ]
  },
  {
   "source": [
    "### See the correlation\n",
    "Actually it is not necessary for 2 columns of data..."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.corr()\n",
    "f, ax = plt.subplots(figsize=(11,9))\n",
    "cmap = sns.diverging_palette(200, 5, as_cmap=True)\n",
    "sns.heatmap(corr, annot=True, cmap=cmap)\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "### Standardize the feature"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler_df = StandardScaler().fit_transform(df)\n",
    "type(scaler_df)#numpy.ndarray\n",
    "df_scaled = pd.DataFrame(scaler_df, columns=df.columns)\n",
    "\n",
    "fig, ax = plt.subplots(1,2,figsize=(15,5))\n",
    "sns.distplot(df['x2'], ax=ax[0], color='green')\n",
    "ax[0].set_title(\"Original\")\n",
    "sns.distplot(df['x2'], ax=ax[1], color='purple')\n",
    "ax[1].set_title(\"Standardized\")\n",
    "plt.show()\n",
    "f.tight_layout()#Cool"
   ]
  },
  {
   "source": [
    "### Algorithm1: k-means clustering"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "score = []\n",
    "range_values = range(1, 20)\n",
    "for i in range_values:\n",
    "    kmeans = KMeans(n_clusters = i)\n",
    "    kmeans.fit(df_scaled)\n",
    "    score.append(kmeans.inertia_)\n",
    "\n",
    "plt.plot(range_values, score, '--')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.show()\n",
    "# We should select the value of k at the point after which the inertia decreases in a linear fashion.\n",
    "# In this case, the k selected is around k = 5."
   ]
  },
  {
   "source": [
    "### Silhouette Coefficient\n",
    "Silhouette coefficient is a way to evaluate the result of clustering."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "\n",
    "for k in range(2, 20):\n",
    "    km = KMeans(n_clusters=k)\n",
    "    preds = km.fit_predict(df_scaled)\n",
    "    centers = km.cluster_centers_\n",
    "\n",
    "    score = silhouette_score(df_scaled, preds, metric='euclidean')\n",
    "    print(\"For k = {}, silhouette score is {}\".format(k, score))\n",
    "# So k = 4"
   ]
  },
  {
   "source": [
    "### Yellowbrick showing the best k"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "km = KMeans()\n",
    "visualizer = KElbowVisualizer(km, k=(2,21), metric='silhouette', timing=False)\n",
    "visualizer.fit(df_scaled)\n",
    "visualizer.poof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.cluster import SilhouetteVisualizer\n",
    "km = KMeans(n_clusters=4)\n",
    "visualizer = SilhouetteVisualizer(km)\n",
    "visualizer.fit(df_scaled)\n",
    "visualizer.poof()"
   ]
  },
  {
   "source": [
    "### Apply k-means\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=4)\n",
    "km.fit(df_scaled)\n",
    "cluster_label = km.labels_\n",
    "\n",
    "df_scaled['KMeans_Label'] = cluster_label\n",
    "f=plt.figure(figsize=(20,20))\n",
    "scatter_cols = ['x1','x2']\n",
    "for i, col in enumerate(scatter_cols):\n",
    "    ax = f.add_subplot(4,4,i+1)\n",
    "\n",
    "sns.scatterplot(x=df_scaled['x1'],y=df_scaled['x2'],hue=df_scaled['KMeans_Label'],palette='Set1')\n",
    "ax.set_title(\"Result\",color='blue')\n",
    "f.tight_layout()"
   ]
  },
  {
   "source": [
    "### Algorithm2: DBSCAN\n",
    "The result of k-means is stupid to a certain degree (for example, the discrete purple dots near the blue parts should not be purple). Therefore, I would like to apply DBSCAN to do density clustering to improve.\n",
    "Two parameter for DBSCAN:\n",
    "- eps: The maximum distance between two samples for one to be considered as in the neighborhood of the other.\n",
    "- min_samples: The number of samples (or total weight) in a neighborhood for a point to be considered as a core point. This includes the point itself."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "db = DBSCAN(eps=0.12, min_samples=5)\n",
    "db.fit(df_scaled)\n",
    "preds = db.fit_predict(df_scaled)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(x=df_scaled['x1'], y=df_scaled['x2'],c=preds,cmap='Paired')\n",
    "plt.title(\"Clusters determined by DBSCAN\")"
   ]
  },
  {
   "source": [
    "### Algorithm3: GMM\n",
    "Now we try GMM for the same dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "gmm = GaussianMixture(n_components=4).fit(df_scaled)\n",
    "labels = gmm.predict(df_scaled)\n",
    "plt.scatter(x=df_scaled['x1'], y=df_scaled['x2'], c=labels, s=40, cmap='viridis')"
   ]
  },
  {
   "source": [
    "## Task2: Credit cards\n",
    "### Import data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./HW3_2_data.csv')\n",
    "# Remove null values (in fact there is not null value);\n",
    "df.dropna(axis=0, inplace=True)\n",
    "# Remove duplicates (in fact, no either);\n",
    "df = df.drop_duplicates()\n",
    "# Briefly check the data\n",
    "df.info()\n",
    "df.describe(include='all')\n",
    "# df.isnull().sum()"
   ]
  },
  {
   "source": [
    "### Data Cleaning and Standardizing\n",
    "By reading the info above, we may remove CUSTID because this property has nothing to do with clustering."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('CUST_ID', axis = 1, inplace = True)\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df)"
   ]
  },
  {
   "source": [
    "### PCA\n",
    "PCA is commonly used for dimensionality reduction by projecting data only onto the first principle components to obtain low-dimension data while preserving or maximizing the variances along the projected direction.\n",
    "Also, in the well-packaged PCA in sklearn, the api automatically cluster the dots by kmeans (i think...)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)   # We need two attributes to visualize the clustering\n",
    "principal = pca.fit_transform(df_scaled)\n",
    "df_pca = pd.DataFrame(data = principal, columns = ['pca1', 'pca2'])\n",
    "df_pca = pd.concat([df_pca, pd.DataFrame({'cluster': labels})], axis=1)\n",
    "# plt.scatter(x=df_pca['pca1'], y=df_pca['pca2'], c=labels, s=10, cmap='viridis')\n",
    "plt.scatter(x=df_pca['pca1'], y=df_pca['pca2'], c=df_pca['cluster'], s=10, cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python392jvsc74a57bd0276a6b94976cb1ef87aba42620e93de15d5cc2a5210344bb094314543b1d5947",
   "display_name": "Python 3.9.2 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "metadata": {
   "interpreter": {
    "hash": "276a6b94976cb1ef87aba42620e93de15d5cc2a5210344bb094314543b1d5947"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}